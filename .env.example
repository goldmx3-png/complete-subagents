# OpenRouter Configuration
OPENROUTER_API_KEY=  # Get from https://openrouter.ai/keys
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# Models (all open-source via OpenRouter)
MAIN_MODEL=mistralai/magistral-small-2506  # Fast 7B model
ROUTER_MODEL=mistralai/magistral-small-2506  # Same model for routing

# Alternative models:
# MAIN_MODEL=meta-llama/llama-3.1-70b-instruct
# MAIN_MODEL=google/gemini-2.0-flash-exp:free  # Free tier!

# LLM Configuration
MAX_TOKENS=4096
TEMPERATURE=0.7
ROUTER_TEMPERATURE=0.3
USE_RULE_BASED=true  # Fast rule-based routing before LLM

# Vector Store
VECTOR_STORE=qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=  # Only for Qdrant Cloud
QDRANT_COLLECTION=documents

# Embeddings
EMBEDDING_MODEL=BAAI/bge-m3
EMBEDDING_DIMENSION=1024
EMBEDDING_DEVICE=cpu  # or cuda
EMBEDDING_BATCH_SIZE=32

# Upload Directory (can be shared across services)
UPLOAD_DIRECTORY=uploads  # or use absolute path like /path/to/shared/uploads

# Reranker (optional)
RERANKER_MODEL=BAAI/bge-reranker-v2-m3
RERANKER_DEVICE=cpu
USE_RERANKER=false

# Database
DATABASE_URL=postgresql://chatbot_user:changeme@localhost:5432/chatbot
DB_POOL_SIZE=20
DB_MAX_OVERFLOW=40

# Application Settings
MAX_CONTEXT_LENGTH=32768
MAX_CHUNKS_PER_QUERY=10
CONVERSATION_HISTORY_LIMIT=10
MAX_FILE_SIZE_MB=100
CHUNK_SIZE=1024
CHUNK_OVERLAP=100

# RAG Settings
MIN_SIMILARITY_SCORE=0.3
AMBIGUITY_THRESHOLD=0.15
TOP_K_RETRIEVAL=5
TOP_K_RERANK=10

# Query Enhancement
USE_QUERY_REWRITING=true
QUERY_REWRITE_CACHE_TTL=86400  # 24 hours

# Agentic RAG Settings (Multi-step RAG with validation and refinement)
AGENTIC_RAG_ENABLED=true                    # Enable/disable agentic RAG
AGENTIC_RAG_MAX_ITERATIONS=3                # Max refinement loops
AGENTIC_RAG_MIN_RELEVANT_DOCS=2             # Min docs to proceed
AGENTIC_RAG_RETRIEVAL_TOP_K=10              # Retrieve more docs for grading
AGENTIC_RAG_TIMEOUT=30000                   # 30 second timeout (ms)
GRADING_CONFIDENCE_THRESHOLD=0.7            # Document relevance threshold
ENABLE_PARALLEL_GRADING=true                # Parallel doc grading (9x faster)
ENABLE_QUERY_CACHE=true                     # Cache common query rewrites
ENABLE_EARLY_EXIT=true                      # Skip validation if confident
AGENTIC_RAG_MIN_QUERY_LENGTH=8              # Min words for agentic path

# API Settings
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
CORS_ORIGINS=http://localhost:3000,http://localhost:8000
LOG_LEVEL=INFO
DEBUG_MODE=false

# Security
API_KEY_REQUIRED=false
ALLOWED_UPLOAD_EXTENSIONS=pdf,docx,txt
MAX_REQUESTS_PER_MINUTE=60

# Banking API Integration (optional)
BANKING_API_JWT_TOKEN=
BANKING_API_TIMEOUT=30
BANKING_API_MAX_RETRIES=3
BANKING_API_VERIFY_SSL=false  # Set to false for internal/dev endpoints with self-signed certificates

# ===== ENHANCED RETRIEVAL (2025) =====

# Hybrid Search Configuration (Vector + BM25)
ENABLE_HYBRID_SEARCH=true  # Combine semantic + keyword search for 8-15% accuracy boost
HYBRID_VECTOR_WEIGHT=0.7  # Weight for vector similarity
HYBRID_BM25_WEIGHT=0.3  # Weight for keyword matching
BM25_K1=1.5  # Term frequency saturation parameter
BM25_B=0.75  # Document length normalization

# Reranking Configuration
ENABLE_RERANKING=true  # Use cross-encoder for final reranking

# Reranker Model Options (in order of speed vs accuracy):
# - cross-encoder/ms-marco-MiniLM-L-6-v2  (FASTEST - ~80MB, 2-3s load time)
# - BAAI/bge-reranker-v2-m3              (BALANCED - ~140MB, 5-8s load time)  [RECOMMENDED]
# - BAAI/bge-reranker-base               (GOOD - ~278MB, 10-15s load time)
# - BAAI/bge-reranker-large              (BEST - ~560MB, 20-30s load time)

RERANKER_MODEL=BAAI/bge-reranker-v2-m3  # Balanced speed/accuracy (recommended)
RERANKER_TOP_K=20  # Retrieve this many docs before reranking
RERANKER_RETURN_TOP_K=5  # Return this many after reranking
RERANKER_DEVICE=cpu  # cpu or cuda
RERANKER_BATCH_SIZE=8  # Batch size for reranking
RERANKER_LOAD_TIMEOUT=300  # Timeout for model loading in seconds (5 minutes default, increase if slow network)

# Token-Based Chunking (for complex documents with tables)
CHUNK_SIZE_TOKENS=600  # Chunk size in tokens (400-800 recommended for banking docs)
CHUNK_OVERLAP_PERCENTAGE=15  # Overlap percentage (10-20% recommended)
USE_TOKEN_BASED_CHUNKING=true  # Use token-based instead of character-based

# Semantic Chunking Configuration
USE_SEMANTIC_CHUNKING=false  # Use LLM to detect logical boundaries
SEMANTIC_CHUNK_MIN_TOKENS=200  # Minimum chunk size
SEMANTIC_CHUNK_MAX_TOKENS=800  # Maximum chunk size
SEMANTIC_CHUNK_MODEL=mistralai/mistral-small-latest  # Model for structure detection
PRESERVE_TABLES=true  # Keep tables as complete chunks when possible

# Markdown-Based Chunking Configuration (with docling PDF parser)
USE_MARKDOWN_CHUNKING=false  # Use docling to convert PDFs to markdown, then chunk by headers
MARKDOWN_CHUNK_SIZE_TOKENS=600  # Chunk size in tokens (400-800 recommended)
MARKDOWN_CHUNK_OVERLAP_PERCENTAGE=15  # Overlap percentage (10-20% recommended)
MARKDOWN_TABLE_SIZE_THRESHOLD=500  # Token threshold: small tables stay inline, large tables become separate chunks
MARKDOWN_PRESERVE_HEADERS=true  # Preserve header hierarchy (h1, h2, h3, h4) in chunk metadata
DOCLING_EXTRACT_TABLES=true  # Extract tables from PDFs during conversion
DOCLING_EXTRACT_IMAGES=false  # Extract images from PDFs (for future enhancement)
